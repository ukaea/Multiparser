{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#multiparser","title":"Multiparser","text":"<p>A parallel multiple file parse trigger system</p> <p>Multiparser is a framework developed by the United Kingdom Atomic Energy Authority focused on tracking changes to files, triggering user-defined callbacks on file creation and modification. This allows the user to monitor the output from multiple processes and define how metrics of interest are handled. The project is available under an MIT license allowing reuse within both open source and proprietary software.</p> <p>Multiparser Example</p> <pre><code>import logging\n\nlogging.basicConfig()\n\nfrom typing import Any\nfrom multiparser import FileMonitor\n\nlogger = logging.getLogger(__file__)\n\ndef callback(data: dict[str, Any], metadata: dict[str, Any]) -&gt; None:\n    logger.info(\"Parsed data: %s\", data)\n\n\nwith FileMonitor(\n    per_thread_callback=callback,\n    timeout=10,\n) as monitor:\n    monitor.tail(path_glob_exprs=\"*.log\", [r\"^completion: (\\d+)%\"], [\"completion\"])\n    monitor.run()\n</code></pre>"},{"location":"basic_example/","title":"Basic Example","text":"<p>Consider the case where a user has set up an RestAPI for receiving metrics across a server. A request should be made only if new values are available and the files being monitored have been modified.</p> <p>First we define a callback which will be executed each time changes are detected:</p> <pre><code>def file_update_callback(data: dict[str, Any], metadata: dict[str, Any]) -&gt; None:\n    _params = {\"data\": data}\n    _response = requests.post(API_ENDPOINT, params=_params)\n</code></pre> <p>Now we initialise a <code>FileMonitor</code> to perform the parallel parsing of any log files from our process, we create an <code>Event</code> to trigger termination of the monitor when the process finishes:</p> <pre><code>trigger = multiprocess.Event()\n\nwith FileMonitor(\n    per_thread_callback=file_update_callback,\n    termination_trigger=trigger,\n    interval=1\n) as monitor:\n</code></pre> <p>The files we are interested in are logs of the form <code>session_X_Y.log</code>, we use the <code>tail</code> method to track any additions to the file since last modification and look for key-value pairs of the form <code>key = value</code>:</p> <pre><code>monitor.track(\n    path_glob_exprs=\"session_*.log\",\n    tracked_values=[r\"(\\w+)\\s*=\\s*([\\d\\w\\.]+)\"]\n)\n</code></pre> <p>finally we set the monitor to run:</p> <pre><code>monitor.run()\n</code></pre> <p>and then set the process to run waiting for it to complete before terminating the monitor with the trigger:</p> <pre><code>subprocess.Popen([\"./process\"], shell=False)\nsubprocess.poll()\ntrigger.set()\n</code></pre> <p>Sending data to a RestAPI</p> <pre><code>import requests\nimport multiparser\nimport multiprocessing\nimport subprocess\n\nAPI_ENDPOINT: str = \"https://api.example.com/v1/metrics\"\n\ntrigger = multiprocessing.Event()\n\ndef file_update_callback(data: dict[str, Any], metadata: dict[str, Any]) -&gt; None:\n    _params = {\"data\": data}\n    _response = requests.post(API_ENDPOINT, params=_params)\n\nwith FileMonitor(\n    per_thread_callback=file_update_callback,\n    timeout=120,\n    termination_trigger=trigger,\n    interval=1\n) as monitor:\n    monitor.track(\n        path_glob_exprs=\"session_*.log\",\n        tracked_values=[r\"(\\w+)\\s*=\\s*([\\d\\w\\.]+)\"]\n    )\n\n    monitor.run()\n\n    subprocess.Popen([\"./process\"], shell=False)\n    subprocess.poll()\n    trigger.set()\n</code></pre>"},{"location":"builtin_parsers/","title":"Parser Functions","text":"<p>Depending on requirement, parsing actions can either be specified using the built-in set of parsers available within Multiparser, or by defining custom parsers.</p> <p>The module comes with a set of pre-defined parsing functions which can be used when processing a recognised file type from the table below:</p> File Suffix Description <code>.json</code> JSON key-value file <code>.toml</code> TOML key-value file <code>.csv</code> Comma separated values file <code>.yaml</code> YAML indent based key-value file <code>.pckl</code>/<code>.pickle</code>/<code>.pkl</code> Pickle file type <code>.nml</code> Fortran named list* <code>.pqt</code>/<code>.parquet</code> Apache open source column-orientated data file** <code>.ft</code>/<code>.feather</code> Apache arrow portable file format** <p>* Requires the extra <code>fortran</code> to be installed.</p> <p>** Requires the extra <code>arrow</code> to be installed.</p> <p>These are executed when using \"tracking\" (see Tracking and Tailing) on recognised file types without custom parser specification. Note they assume the data can be loaded as a single level dictionary.</p>"},{"location":"custom_parsers/","title":"Custom Parsers","text":"<p>In the situation where the output files from a process are not processable by any of the built-in parsers, a custom parser can be created to extract the information of interest.</p>"},{"location":"custom_parsers/#file-parsers","title":"File Parsers","text":"<p>File parsers are used for tracking, they take the path of an identified candidate as an argument, parse the data from that file and return a key-value mapping of the data of interest. To create a custom parser you will need to use the <code>multiparser.parsing.file.file_parser</code> decorator. The function should take an argument <code>input_file</code> which is the file path, and allow an arbitrary number of additional arguments (<code>**_</code>) to be compatible with the decorator. It should return either:</p> <ul> <li>Two dictionaries containing relevant metadata (usually left blank), and the parsed information: <code>{...}, {...}</code>.</li> <li>A single dictionary representing the metadata, and a list of dictionaries (for cases where multiple lines are read in a single parse and these should be kept separate): <code>{...}, [{...}, ...]</code></li> </ul> <pre><code>from typing import Any\nimport multiparser.parsing.file as mp_file_parse\n\n@mp_file_parse.file_parser\ndef parse_user_file_type(input_file: str, **_) -&gt; tuple[dict[str, Any], dict[str, Any]]:\n    with open(file_path) as in_f:\n        file_lines = in_f.readlines()\n\n    data = {}\n\n    for line in file_lines:\n        key, value = line.split(\":\", 1)\n        data[key.strip().lower().replace(\" \", \"_\")] = value.strip()\n\n    return {}, data\n</code></pre> <p>To use the parser within a <code>FileMonitor</code> session:</p> <pre><code>with multiparser.FileMonitor(timeout=10) as monitor:\n    monitor.track(path_glob_exprs=\"custom_file.log\", parser_func=parser_user_file_type)\n    monitor.run()\n</code></pre> <p>In the case where you would like your parser function to accept additional keyword arguments you can add these to the definition and pass them to tracking using the <code>parser_kwargs</code> argument.</p> <p>Empty Data</p> <p>The <code>per_thread_callback</code> of <code>FileMonitor</code> only executes when new data is parsed from a modified file, not when the file is marked as modified. Therefore a custom parser which either does not return data, or returns explicitly an empty dictionary will never trigger the <code>per_thread_callback</code> method.</p>"},{"location":"custom_parsers/#log-parsers","title":"Log Parsers","text":"<p>In the case where the custom parser will be used in file \"tailing\", that is read only the latest information appended to the file, the <code>multiparser.parsing.tail.log_parser</code> decorator is used when defining the function. The function should take an argument <code>file_content</code> which is a string containing the latest read content, and allow an arbitrary number of additional arguments (<code>**_</code>) to be compatible with the decorator. It should return either:</p> <ul> <li>Two dictionaries containing relevant metadata (usually left blank), and the parsed information: <code>{...}, {...}</code>.</li> <li>A single dictionary representing the metadata, and a list of dictionaries (for cases where multiple lines are read in a single parse and these should be kept separate: <code>{...}, [{...}, ...]</code>.</li> </ul> <pre><code>from typing import Any\nimport multiparser.parsing.tail as mp_tail_parse\n\n@mp_tail_parse.log_parser\ndef parse_user_file_type(file_content: str, **_) -&gt; tuple[dict[str, Any], dict[str, Any]]:\n    file_lines = file_content.split(\"\\n\")\n\n    data = {}\n\n    for line in file_lines:\n        key, value = line.split(\":\", 1)\n        data[key.strip().lower().replace(\" \", \"_\")] = value.strip()\n\n    return {}, data\n</code></pre> <p>To use the parser within a <code>FileMonitor</code> session:</p> <pre><code>with multiparser.FileMonitor(timeout=10) as monitor:\n    monitor.tail(path_glob_exprs=\"custom_file.log\", parser_func=parser_user_file_type)\n    monitor.run()\n</code></pre> <p>In the case where you would like your parser function to accept additional keyword arguments you can add these to the definition and pass them to tracking using the <code>parser_kwargs</code> argument.</p>"},{"location":"custom_parsers/#class-methods-as-parsers","title":"Class methods as parsers","text":"<p>When custom parsers are decorated any positional and keyword arguments are propagated through, i.e.:</p> <pre><code>@log_parser\ndef my_parser(a_position_arg, file_content, a_keyword_argument):\n  ...\n</code></pre> <p>will be called as:</p> <pre><code>my_parser(*args, file_content=file_content, **kwargs)\n</code></pre> <p>This allows class methods to also work as custom parsers because <code>self</code> is handled correctly:</p> <pre><code>class MyClass:\n  def __init__(self):\n    ...\n\n  @log_parser\n  def my_parser(self, file_content):\n    ...\n</code></pre> <p>Empty Data</p> <p>The <code>per_thread_callback</code> of <code>FileMonitor</code> only executes when new data is parsed from a modified file, not when the file is marked as modified. Therefore a custom parser which either does not return data, or returns explicitly an empty dictionary will never trigger the <code>per_thread_callback</code> method.</p>"},{"location":"development/","title":"Development","text":""},{"location":"development/#using-uv","title":"Using UV","text":"<p>The Multiparser repository makes use of the UV which is a pip-installable dependency management and virtual environment tool for assisting development, and recommended when contributing to the project. The included <code>uv.lock</code> file provides a shareable virtual environment definition. UV is able to resolve dependency versions to ensure cross-compatibility.</p>"},{"location":"development/#pre-commit","title":"Pre-commit","text":"<p>The repository includes a pre-commit configuration file which can be used to setup git hooks executed during committing. To install the hooks firstly ensure pre-commit is installed then run:</p> <pre><code>pre-commit install\n</code></pre> <p>within the repository. You can manually run all hooks by executing:</p> <pre><code>pre-commit run --all\n</code></pre>"},{"location":"development/#documentation","title":"Documentation","text":"<p>Multiparser follows the Numpy docstring convention for outlining function and class parameters and return types.</p> <p>Included within the git hooks is the docstring coverage check tool interrogate which checks whether all functions within the <code>multiparser</code> module have been documented. The tool can be run in isolation using UV:</p> <pre><code>uv run interrogate\n</code></pre>"},{"location":"file_monitor/","title":"<code>FileMonitor</code>","text":"<p>The <code>FileMonitor</code> class is the basis of all Multiparser sessions, it allows the user to define what files are tracked, and what to do when a change is detected. The class must be used as a context manager with the Python <code>with</code> statement:</p> <pre><code>with multiparser.FileMonitor(\n    per_thread_callback,\n    exception_callback,\n    notification_callback,\n    termination_trigger,\n    timeout,\n    lock_callbacks\n) as file_monitor:\n    ...\n</code></pre>"},{"location":"file_monitor/#arguments","title":"Arguments","text":""},{"location":"file_monitor/#per_thread_callback","title":"<code>per_thread_callback</code>","text":"<p><code>Callable[[dict[str, Any], dict[str, Any]], None] | None</code></p> <p>Default: <code>None</code></p> <p>A callback function executed when changes are detected on any monitored file, this can also be overwritten on a per file basis. The function must accept the arguments <code>data</code> and <code>metadata</code>, e.g.:</p> <pre><code>def my_callback(data: dict[str, Any], metadata: dict[str, Any]) -&gt; None:\n    logging.getLogger(\"Simulation\").info(f\"Recorded {data}\")\n</code></pre> <p>This argument is optional, although a recommended starting point.</p>"},{"location":"file_monitor/#exception_callback","title":"<code>exception_callback</code>","text":"<p><code>Callable[[str], None] | None</code></p> <p>Default: <code>None</code></p> <p>An optional callback which is handed an exception message which is an amalgamation of all exceptions for running file monitor threads, e.g.:</p> <pre><code>def exception_callback(exception_msg: str) -&gt; None:\n    logging.getLogger(\"Simulation\").error(exception_msg)\n</code></pre>"},{"location":"file_monitor/#notification_callback","title":"<code>notification_callback</code>","text":"<p><code>Callable[[str], None] | None</code></p> <p>Default: <code>None</code></p> <p>An optional callback which is handed the name of a file upon initial detection, e.g.:</p> <pre><code>def notification_callback(file_name: str) -&gt; None:\n    logging.getLogger(\"Simulation\").info(f\"Found new file: {file_name}\")\n</code></pre> <p>Note this already has a default of informing the user of any new files that have been discovered.</p>"},{"location":"file_monitor/#termination_trigger","title":"<code>termination_trigger</code>","text":"<p><code>multiprocessing.synchronize.Event | None</code></p> <p>Default: <code>None</code></p> <p>A <code>multiprocessing.Event</code> object, if specified the <code>FileMonitor</code> will run indefinitely until the trigger is <code>set</code>:</p> <pre><code>import multiprocessing\n\ntrigger = multiprocessing.Event()\n</code></pre> <p>Not needed if <code>timeout</code> is specified. Note if both <code>timeout</code> and <code>termination_trigger</code> are specified, termination will occur when either the trigger is set externally, or the timeout period is reached.</p>"},{"location":"file_monitor/#subprocess_triggers","title":"<code>subprocess_triggers</code>","text":"<p><code>list[multiprocessing.synchronize.Event] | None</code></p> <p>Default: <code>None</code></p> <p>If specified, these are <code>multiprocessing.Event</code> objects which are <code>set</code> by the <code>FileMonitor</code> itself when it is terminated.</p>"},{"location":"file_monitor/#timeout","title":"<code>timeout</code>","text":"<p><code>float | int | None</code></p> <p>Default: <code>None</code></p> <p>In a case where <code>termination_trigger</code> cannot be specified this is the time in seconds the <code>FileMonitor</code> will run before timing out. Note if both <code>timeout</code> and <code>termination_trigger</code> are specified, termination will occur when either the trigger is set externally, or the timeout period is reached.</p>"},{"location":"file_monitor/#lock_callbacks","title":"<code>lock_callbacks</code>","text":"<p><code>bool</code></p> <p>Default: <code>False</code></p> <p>Whether to only allow a single file monitoring thread to execute the callback at a given time. Uses a mutex to prevent the callback being made by two threads at the same time.</p>"},{"location":"file_monitor/#interval","title":"<code>interval</code>","text":"<p><code>float</code></p> <p>Default: <code>0.1</code></p> <p>File monitoring interval, i.e. how often the thread monitoring a file should check for any updates, the default is <code>0.1</code> seconds.</p>"},{"location":"file_monitor/#log_level","title":"<code>log_level</code>","text":"<p><code>str | int</code></p> <p>Default: <code>logging.INFO</code></p> <p>Logger level for the <code>FileMonitor</code>, default is <code>logging.INFO</code>, for more information and the display recorded data set to <code>logging.DEBUG</code>.</p>"},{"location":"file_monitor/#flatten_data","title":"<code>flatten_data</code>","text":"<p><code>bool</code></p> <p>Default: <code>False</code></p> <p>By default Multiparser will pass the data mapping assigning the result 'as is' as an argument to the specified callback. Alternatively if <code>flatten_data</code> is set to <code>True</code> a delimiter <code>.</code> is used to flatten the data into single level key-value pairs.</p> <pre><code># Before\ndata = {\n    \"contents\": {\n        \"car\": \"ford\",\n        \"pet\": \"dog\",\n        \"house\": \"chalet\"\n    }\n}\n\n# After\n\ndata = {\n    \"contents.car\": \"ford\",\n    \"contents.pet\": \"dog\",\n    \"contents.house\": \"chalet\n}\n</code></pre>"},{"location":"file_monitor/#plain_logging","title":"<code>plain_logging</code>","text":"<p><code>bool</code></p> <p>Default: <code>False</code></p> <p>Disable the color formatted logger statements replacing them with plain text only.</p>"},{"location":"file_monitor/#terminate_all_on_failure","title":"<code>terminate_all_on_failure</code>","text":"<p><code>bool</code></p> <p>Default: <code>False</code></p> <p>If set all file threads are terminated when one fails, i.e. all activity is ceased in the case where a thread throws an exception.</p>"},{"location":"file_monitor/#file_limit","title":"<code>file_limit</code>","text":"<p><code>int | None</code></p> <p>Default: <code>50</code></p> <p>The number of allowed concurrent running threads for each of the two file monitor types, track and tail. If <code>None</code> then there is no limit.</p>"},{"location":"how_it_works/","title":"How it Works","text":"<p>Multiparser is designed to watch a set of paths for changes, parsing these additions and executing any user specified callbacks to handle them.</p> <p></p>"},{"location":"installation/","title":"Installation","text":"<p>Multiparser requires Python 3.10+ and is installed using pip:</p> <pre><code>pip install ukaea-multiparser\n</code></pre> <p>Multiparser has two optional extra components that can be installed:</p> <ul> <li><code>arrow</code>: Adds compatibility with Apache Arrow file types.</li> <li><code>fortran</code>: Adds parsing of Fortran named lists.</li> </ul> <p>To install multiparser with any of these extras:</p> <p><pre><code>pip install ukaea-multiparser[arrow]\n</code></pre> <pre><code>pip install ukaea-multiparser[fortran]\n</code></pre> <pre><code>pip install ukaea-multiparser[arrow,fortran]\n</code></pre></p>"},{"location":"tailing/","title":"<code>FileMonitor.tail</code>","text":"<p>The <code>FileMonitor</code> method <code>tail</code> is used to monitor appended changes to files, that is content which has been added to the file incrementally, for example log files containing the <code>stdout</code> of a given process.</p> <pre><code>monitor.tail(\n    path_glob_exprs=\"*.log\",\n    tracked_values=[re.compile(r'^(\\w+)=(\\d+))', re.compile(r'\\w+=([\\d\\.]+)')],\n    labels=[None, \"fraction\"],\n    callback=lambda data, _: logging.getLogger(\"Simulation\").info(data),\n    parser_func=None,\n    parser_kwargs=None\n)\n</code></pre>"},{"location":"tailing/#arguments","title":"Arguments","text":""},{"location":"tailing/#path_glob_exprs","title":"<code>path_glob_exprs</code>","text":"<p><code>list[str] | str</code></p> <p>Either a single globular expression string or a list of such strings defining files to be monitored by this tail, can be an explicit string if only one file is required.</p>"},{"location":"tailing/#tracked_values","title":"<code>tracked_values</code>","text":"<p><code>list[str | Pattern] | str | Pattern | None</code></p> <p>Default: <code>None</code></p> <p>Either a single string or regular expression, or a list of a combination of either defining values to be recorded. In general a regular expression applies here as it provides a pattern for recording, a string may be used if waiting for the appearance of a statement explicitly. Regular expressions must be <code>re.compile</code> objects.</p> <p>Regular expressions can have either one capture group representing the value to capture, or two representing the key and the value. Note when only one regex group or a string is specified a label must also be present (see <code>labels</code>).</p> <pre><code>tracked_values = [\n    \"END OF LINE\",  # Wait for exact string (requires label)\n    re.compile(r'(\\w+)=(\\d+)'), # Specify label/key and value to capture\n    re.compile(r'\\w+=(\\d+)') # Specify only value to capture (requires label)\n]\n</code></pre>"},{"location":"tailing/#skip_lines_w_pattern","title":"<code>skip_lines_w_pattern</code>","text":"<p><code>list[Pattern | str] | None</code></p> <p>Default: <code>None</code></p> <p>A list of patterns or strings defining lines to exclude when parsing a file. The patterns are <code>re.compile</code> objects as above:</p> <pre><code>skip_lines_w_pattern = [\"# Header\", re.compile(r'^# Begin (\\w+)')]\n</code></pre>"},{"location":"tailing/#labels","title":"<code>labels</code>","text":"<p><code>list[str] | None</code></p> <p>Default: <code>None</code></p> <p>Labels define the key associated with each captured value, in the case where either a string or a single regex capture group is used (see <code>tracked_values</code>) this is mandatory, else in other cases it will overwrite the capture group:</p> <pre><code>tracked_values = [\n    \"END OF LINE\",  # Wait for exact string (requires label)\n    re.compile(r'(\\w+)=(\\d+)'), # Specify label/key and value to capture\n    re.compile(r'\\w+=(\\d+)') # Specify only value to capture (requires label)\n]\n\n# Scenario 1: Assign labels to first and last tracked_values definition\n# labels for first and last entries are mandatory\nlabels = [\"eof\", None, \"key\"]\n\n# Scenario 2: As above but overwrite key captured by regex in second entry\nlabels = [\"eof\", \"new_key\", \"key\"]\n</code></pre>"},{"location":"tailing/#callback","title":"<code>callback</code>","text":"<p><code>Callable[[dict[str, Any], dict[str, Any]], None] | None</code></p> <p>Default: <code>None</code></p> <p>Optional callback exclusively for this file set, this overwrites the global callback (if specified) within the <code>FileMonitor</code> definition (see here).</p>"},{"location":"tailing/#parser_func","title":"<code>parser_func</code>","text":"<p><code>Callable[[str, *_], tuple[dict[str, Any], dict[str, Any]]] | None</code></p> <p>Default: <code>None</code></p> <p>Specific a custom parser for this file set, this must be a log parsing function wrapped in the <code>multiparser.parsing.tail.log_parser</code> decorator, for more information see here.</p>"},{"location":"tailing/#parser_kwargs","title":"<code>parser_kwargs</code>","text":"<p><code>dict[str, Any] | None</code></p> <p>Default: <code>None</code></p> <p>Keyword arguments to pass to the function defined above, useful if you want to specify near-identical parsing of multiple file sets with slight customisation.</p>"},{"location":"tailing/#returns","title":"Returns","text":"<p><code>None</code></p>"},{"location":"tracking/","title":"<code>FileMonitor.track</code>","text":"<p>The <code>FileMonitor</code> method <code>track</code> is used to monitor files as a whole, that is loading the full file whenever it is modified and parsing it in its entirety, for example JSON or YAML files.</p> <pre><code>monitor.track(\n    path_glob_exprs=\"*.log\",\n    tracked_values=[re.compile(r'^(\\w+)=(\\d+))', re.compile(r'\\w+=([\\d\\.]+)')],\n    labels=[None, \"fraction\"],\n    callback=lambda data, _: logging.getLogger(\"Simulation\").info(data),\n    parser_func=None,\n    parser_kwargs=None\n)\n</code></pre>"},{"location":"tracking/#arguments","title":"Arguments","text":""},{"location":"tracking/#path_glob_exprs","title":"<code>path_glob_exprs</code>","text":"<p><code>list[str] | str</code></p> <p>Either a single globular expression string or a list of such strings defining files to be monitored by this tail, can be an explicit string if only one file is required.</p>"},{"location":"tracking/#tracked_values","title":"<code>tracked_values</code>","text":"<p><code>list[str | Pattern] | str | Pattern | None</code></p> <p>Default: <code>None</code></p> <p>Either a single string or regular expression, or a list of a combination of either defining keys for values to be recorded. Regular expressions must be <code>re.compile</code> objects. In the case of regular expressions <code>findall</code> is used to filter required information. If no tracked values are specified all content is read.</p> <pre><code>tracked_values = [\n    \"name\",\n    re.compile(r'var_(\\w+)'),\n]\n</code></pre>"},{"location":"tracking/#callback","title":"<code>callback</code>","text":"<p><code>Callable[[dict[str, Any], dict[str, Any]], None] | None</code></p> <p>Default: <code>None</code></p> <p>Optional callback exclusively for this file set, this overwrites the global callback (if specified) within the <code>FileMonitor</code> definition (see here).</p>"},{"location":"tracking/#parser_func","title":"<code>parser_func</code>","text":"<p><code>Callable[[str, *_], tuple[dict[str, Any], dict[str, Any]]] | None</code></p> <p>Default: <code>None</code></p> <p>Specific a custom parser for this file set, this must be a file parsing function wrapped in the <code>multiparser.parsing.file.file_parser</code> decorator, for more information see here.</p>"},{"location":"tracking/#parser_kwargs","title":"<code>parser_kwargs</code>","text":"<p><code>dict[str, Any] | None</code></p> <p>Default: <code>None</code></p> <p>Keyword arguments to pass to the function defined above, useful if you want to specify near-identical parsing of multiple file sets with slight customisation.</p>"},{"location":"tracking/#static","title":"<code>static</code>","text":"<p><code>bool</code></p> <p>Default: <code>False</code></p> <p>If <code>True</code> the file is read only once, recommended if a recorded file is never overwritten later during process execution as it will terminate the thread monitoring that file.</p>"},{"location":"tracking/#file_type","title":"<code>file_type</code>","text":"<p><code>Literal['csv', 'pkl', 'json', 'toml', 'yaml', 'nml', 'pqt', 'ft'] | None</code></p> <p>Default: <code>None</code></p> <p>If using \"lazy\" parsing whereby the file is read based on extension, override the default. This is mandatory whereby the input file suffix does not match any of the above. For more information on built-in parsers see here.</p>"},{"location":"tracking_and_tailing/","title":"Tracking and Tailing Files","text":"<p>In Multiparser there are two main ways in which changes to a file can be monitored, tracking which focuses on revisions of a file and tailing which looks at incremental additions.</p>"},{"location":"tracking_and_tailing/#tracking","title":"Tracking","text":"<p>When a file is tracked it is read in its entirety each time a change is dedicated. For example in the case of JSON and TOML files the data are held as key-value pairs which may be modified during the execution of a process.</p> <pre><code>monitor.track(\n    path_glob_exprs=\"*.json\",\n    tracked_values=[\"title\", \"genre\"]\n)\n</code></pre>"},{"location":"tracking_and_tailing/#tailing","title":"Tailing","text":"<p>When a file is tailed only the most recent additions to the file are read, for example in the case of log files we are only interested in the most recent lines written to the file and can ignore those already parsed previously. This is particularly important when reading large outputs where logs can be kilobytes or even megabytes in size.</p> <pre><code>monitor.tail(\n    path_glob_exprs=\"*.log\",\n    tracked_values=[r\"(\\w+)=([\\d\\.]+)]\n)\n</code></pre>"},{"location":"tracking_and_tailing/#excluding-files","title":"Excluding files","text":"<p>Files can be excluded from monitoring using the <code>exclude</code> method of <code>FileMonitor</code> with either a list or string:</p> <pre><code>monitor.exclude(\n  [\"test*.log\"]\n)\n</code></pre>"}]}